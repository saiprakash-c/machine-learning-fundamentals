{"cells":[{"cell_type":"markdown","metadata":{"id":"UfBRkiRAUQBp"},"source":["## Goals of the project:\n","- Understanding tensors\n","- Autograd\n","- Backpropagation\n","- Loss functions\n","- Optimizers\n","- Data Loading\n","- CPU vs GPU computation\n","- Debugging"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4533,"status":"ok","timestamp":1703144494260,"user":{"displayName":"Sai P","userId":"15285502759182123980"},"user_tz":480},"id":"zA3s382NUQBr","outputId":"81398539-6dcb-44b3-9c97-693dc7e65d61"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(4.)\n","torch.Size([])\n","torch.float32\n","tensor([[[11., 12., 13.],\n","         [13., 14., 15.]],\n","\n","        [[15., 16., 17.],\n","         [17., 18., 19.]]])\n","torch.Size([2, 2, 3])\n","torch.float32\n","tensor([5, 6, 7, 8])\n","torch.Size([4])\n","torch.int64\n","dy/dx: None\n","dy/dw: tensor(3.)\n","dy/db: tensor(1.)\n"]}],"source":["# Understanding tensors\n","# Tensors are similar to numpy's ndarrays, with the addition being that\n","# tensors can also be used on a GPU to accelerate computing\n","# Tensors are also optimized for automatic differentiation (more on that later)\n","# Tensors are also used to encode the inputs and outputs of a model, as well\n","# as the model's parameters\n","\n","# Import torch and other required modules\n","import torch\n","\n","# create a tensor with a single number\n","t1 = torch.tensor(4.)\n","print(t1)\n","print(t1.shape)\n","print(t1.dtype)\n","\n","# create a 3D tensor with 2 matrices (note: 19. is a float)\n","t4 = torch.tensor([[[11, 12, 13], [13, 14, 15]],\n","                   [[15, 16, 17], [17, 18, 19.]]])\n","print(t4)\n","print(t4.shape)\n","print(t4.dtype)\n","\n","# Can tensor store integers? yes\n","t5 = torch.tensor([5, 6, 7, 8])\n","print(t5)\n","print(t5.shape)\n","print(t5.dtype)\n","\n","\n","# Tensor operations and gradients\n","# requires_grad param tells PyTorch that it should track the gradients of the tensor\n","# while we perform operations on it. This way, PyTorch can later perform backpropagation\n","# to calculate the gradients of the cost with respect to W and b\n","# To determine if a tensor is being tracked by gradient descent, check its\n","# requires_grad attribute\n","\n","# Addition\n","x = torch.tensor(3.)\n","w = torch.tensor(4., requires_grad=True)\n","b = torch.tensor(5., requires_grad=True)\n","y = w * x + b\n","\n","\n","# The gradient for this tensor will be accumulated into .grad attribute\n","# We need to set the .grad attribute to zero before calling .backward()\n","# because PyTorch accumulates the gradients on subsequent backward passes\n","# (i.e. the .grad values are added to whatever already exists, rather than\n","# replacing them)\n","# See https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n","\n","# compute gradients\n","y.backward()\n","\n","# display gradients\n","print('dy/dx:', x.grad)\n","print('dy/dw:', w.grad)\n","print('dy/db:', b.grad)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uN90RHXCUQBt"},"outputs":[],"source":["# Autograd is an automatic differentiation tool in PyTorch\n","# It calculates the gradients of your parameters with respect to a loss function\n","# The autograd package provides automatic differentiation for all operations on Tensors\n","# It is a define-by-run framework, which means that your backprop is defined by how your\n","# code is run, and that every single iteration can be different\n","# If you set the attribute .requires_grad as True, it starts to track all operations on it\n","# When you finish your computation you can call .backward() and have all the gradients computed automatically\n","# The gradient for this tensor will be accumulated into .grad attribute"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1703144494261,"user":{"displayName":"Sai P","userId":"15285502759182123980"},"user_tz":480},"id":"wUNyt8aqUQBu","outputId":"7756c574-8fa2-40a3-a301-aaf78574de9b"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor_data[0, 0, 0, 0]: tensor(1)\n","tensor_data[0, 0, 0, :]: tensor([1, 2, 3, 4, 5])\n","tensor_data[0, 0, :, 0]: tensor([ 1,  6, 11, 16])\n","tensor_data[0, 0, :, :]: tensor([[ 1,  2,  3,  4,  5],\n","        [ 6,  7,  8,  9, 10],\n","        [11, 12, 13, 14, 15],\n","        [16, 17, 18, 19, 20]])\n","tensor_data[0, :, :, :]: tensor([[[ 1,  2,  3,  4,  5],\n","         [ 6,  7,  8,  9, 10],\n","         [11, 12, 13, 14, 15],\n","         [16, 17, 18, 19, 20]],\n","\n","        [[21, 22, 23, 24, 25],\n","         [26, 27, 28, 29, 30],\n","         [31, 32, 33, 34, 35],\n","         [36, 37, 38, 39, 40]],\n","\n","        [[41, 42, 43, 44, 45],\n","         [46, 47, 48, 49, 50],\n","         [51, 52, 53, 54, 55],\n","         [56, 57, 58, 59, 60]]])\n","tensor_data[:, :, :, :]: tensor([[[[  1,   2,   3,   4,   5],\n","          [  6,   7,   8,   9,  10],\n","          [ 11,  12,  13,  14,  15],\n","          [ 16,  17,  18,  19,  20]],\n","\n","         [[ 21,  22,  23,  24,  25],\n","          [ 26,  27,  28,  29,  30],\n","          [ 31,  32,  33,  34,  35],\n","          [ 36,  37,  38,  39,  40]],\n","\n","         [[ 41,  42,  43,  44,  45],\n","          [ 46,  47,  48,  49,  50],\n","          [ 51,  52,  53,  54,  55],\n","          [ 56,  57,  58,  59,  60]]],\n","\n","\n","        [[[ 61,  62,  63,  64,  65],\n","          [ 66,  67,  68,  69,  70],\n","          [ 71,  72,  73,  74,  75],\n","          [ 76,  77,  78,  79,  80]],\n","\n","         [[ 81,  82,  83,  84,  85],\n","          [ 86,  87,  88,  89,  90],\n","          [ 91,  92,  93,  94,  95],\n","          [ 96,  97,  98,  99, 100]],\n","\n","         [[101, 102, 103, 104, 105],\n","          [106, 107, 108, 109, 110],\n","          [111, 112, 113, 114, 115],\n","          [116, 117, 118, 119, 120]]]])\n"]}],"source":["# Reduction operations\n","\n","# Indexing, slicing, joining, mutating ops\n","\n","# create a 4d tensor with C=2, H=3, W=4, D=5\n","import torch\n","\n","# Example numbers for a tensor of shape (2, 3, 4, 5)\n","tensor_data = torch.tensor([\n","    [\n","        [\n","            [1, 2, 3, 4, 5],\n","            [6, 7, 8, 9, 10],\n","            [11, 12, 13, 14, 15],\n","            [16, 17, 18, 19, 20]\n","        ],\n","        [\n","            [21, 22, 23, 24, 25],\n","            [26, 27, 28, 29, 30],\n","            [31, 32, 33, 34, 35],\n","            [36, 37, 38, 39, 40]\n","        ],\n","        [\n","            [41, 42, 43, 44, 45],\n","            [46, 47, 48, 49, 50],\n","            [51, 52, 53, 54, 55],\n","            [56, 57, 58, 59, 60]\n","        ]\n","    ],\n","    [\n","        [\n","            [61, 62, 63, 64, 65],\n","            [66, 67, 68, 69, 70],\n","            [71, 72, 73, 74, 75],\n","            [76, 77, 78, 79, 80]\n","        ],\n","        [\n","            [81, 82, 83, 84, 85],\n","            [86, 87, 88, 89, 90],\n","            [91, 92, 93, 94, 95],\n","            [96, 97, 98, 99, 100]\n","        ],\n","        [\n","            [101, 102, 103, 104, 105],\n","            [106, 107, 108, 109, 110],\n","            [111, 112, 113, 114, 115],\n","            [116, 117, 118, 119, 120]\n","        ]\n","    ]\n","])\n","\n","# Examples of slicing the tensor\n","print(\"tensor_data[0, 0, 0, 0]:\", tensor_data[0, 0, 0, 0])\n","print(\"tensor_data[0, 0, 0, :]:\", tensor_data[0, 0, 0, :])\n","print(\"tensor_data[0, 0, :, 0]:\", tensor_data[0, 0, :, 0])\n","print(\"tensor_data[0, 0, :, :]:\", tensor_data[0, 0, :, :])\n","print(\"tensor_data[0, :, :, :]:\", tensor_data[0, :, :, :])\n","print(\"tensor_data[:, :, :, :]:\", tensor_data[:, :, :, :])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1703144494261,"user":{"displayName":"Sai P","userId":"15285502759182123980"},"user_tz":480},"id":"EkOunqzlUQBu","outputId":"c693a52e-4197-4eaa-e47b-3eb2ed64b204"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.cat([t1, t2], dim=0):\n"," tensor([[1, 2],\n","        [3, 4],\n","        [5, 6],\n","        [7, 8]])\n","torch.cat([t1, t2], dim=0).shape:\n"," torch.Size([4, 2])\n","=========================================\n","torch.cat([t1, t2], dim=1):\n"," tensor([[1, 2, 5, 6],\n","        [3, 4, 7, 8]])\n","torch.cat([t1, t2], dim=1).shape:\n"," torch.Size([2, 4])\n","=========================================\n","torch.stack([t1, t2], dim=0):\n"," tensor([[[1, 2],\n","         [3, 4]],\n","\n","        [[5, 6],\n","         [7, 8]]])\n","torch.stack([t1, t2], dim=0).shape:\n"," torch.Size([2, 2, 2])\n","=========================================\n","torch.stack([t1, t2], dim=1):\n"," tensor([[[1, 2],\n","         [5, 6]],\n","\n","        [[3, 4],\n","         [7, 8]]])\n","torch.stack([t1, t2], dim=1).shape:\n"," torch.Size([2, 2, 2])\n","=========================================\n","torch.stack([t1, t2], dim=2):\n"," tensor([[[1, 5],\n","         [2, 6]],\n","\n","        [[3, 7],\n","         [4, 8]]])\n","torch.stack([t1, t2], dim=2).shape:\n"," torch.Size([2, 2, 2])\n","=========================================\n"]}],"source":["# Examples of joining tensors\n","# Concatenation joins tensors along an existing axis\n","# Stacking joins tensors along a new axis\n","\n","# create two tensors for concatenation and stacking\n","t1 = torch.tensor([[1, 2], [3, 4]])\n","t2 = torch.tensor([[5, 6], [7, 8]])\n","\n","# concatenate on axis 0 (rows)\n","print(\"torch.cat([t1, t2], dim=0):\\n\", torch.cat([t1, t2], dim=0))\n","print(\"torch.cat([t1, t2], dim=0).shape:\\n\", torch.cat([t1, t2], dim=0).shape)\n","print(\"=========================================\")\n","\n","# concatenate on axis 1 (columns)\n","print(\"torch.cat([t1, t2], dim=1):\\n\", torch.cat([t1, t2], dim=1))\n","print(\"torch.cat([t1, t2], dim=1).shape:\\n\", torch.cat([t1, t2], dim=1).shape)\n","print(\"=========================================\")\n","\n","# stack on axis 0 (new dimension)\n","print(\"torch.stack([t1, t2], dim=0):\\n\", torch.stack([t1, t2], dim=0))\n","print(\"torch.stack([t1, t2], dim=0).shape:\\n\", torch.stack([t1, t2], dim=0).shape)\n","print(\"=========================================\")\n","\n","# stack on axis 1 (new dimension)\n","print(\"torch.stack([t1, t2], dim=1):\\n\", torch.stack([t1, t2], dim=1))\n","print(\"torch.stack([t1, t2], dim=1).shape:\\n\", torch.stack([t1, t2], dim=1).shape)\n","print(\"=========================================\")\n","\n","# stack on axis 2 (new dimension)\n","print(\"torch.stack([t1, t2], dim=2):\\n\", torch.stack([t1, t2], dim=2))\n","print(\"torch.stack([t1, t2], dim=2).shape:\\n\", torch.stack([t1, t2], dim=2).shape)\n","print(\"=========================================\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":806,"status":"ok","timestamp":1703144495064,"user":{"displayName":"Sai P","userId":"15285502759182123980"},"user_tz":480},"id":"45YLYk8XUQBu","outputId":"21b3b009-d7ea-40ad-bc38-387df3f655ec"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.min(t, dim=0): tensor([[1, 1, 4],\n","        [3, 7, 3]])\n","torch.argmin(t, dim=0): tensor([[0, 1, 1],\n","        [1, 1, 0]])\n","torch.sum(t, dim=0): tensor([[ 3,  4,  9],\n","        [13, 22,  9]])\n","torch.prod(t, dim=0): tensor([[  2,   3,  20],\n","        [ 30, 105,  18]])\n"]}],"source":["# Reduction operations\n","# Min, max, mean, argmax, argmin, sum, prod\n","\n","# create a 3d tensor\n","t = torch.tensor([\n","    [\n","        [1, 3, 5],\n","        [10, 15, 3]\n","    ],\n","    [\n","        [2, 1, 4],\n","        [3, 7, 6]\n","    ]\n","])\n","\n","# find the minimum value along axis 0 (rows)\n","print(\"torch.min(t, dim=0):\", torch.min(t, dim=0).values)\n","\n","# argmin returns the index location of the minimum value along axis 0 (rows)\n","print(\"torch.argmin(t, dim=0):\", torch.argmin(t, dim=0))\n","\n","# sum the values along axis 0 (rows)\n","print(\"torch.sum(t, dim=0):\", torch.sum(t, dim=0))\n","\n","# prod the values along axis 0 (rows)\n","print(\"torch.prod(t, dim=0):\", torch.prod(t, dim=0))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1703144495065,"user":{"displayName":"Sai P","userId":"15285502759182123980"},"user_tz":480},"id":"mBejD-ooUQBu","outputId":"1de38fa5-a3c7-4d19-d9e5-fb228582671c"},"outputs":[{"name":"stdout","output_type":"stream","text":["t.reshape(2, 4):\n"," tensor([[1, 2, 3, 4],\n","        [5, 6, 7, 8]])\n","t.reshape(4, 2):\n"," tensor([[1, 2],\n","        [3, 4],\n","        [5, 6],\n","        [7, 8]])\n","t.reshape(8):\n"," tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","t.view(2, 4):\n"," tensor([[1, 2, 3, 4],\n","        [5, 6, 7, 8]])\n","t.view(4, 2):\n"," tensor([[1, 2],\n","        [3, 4],\n","        [5, 6],\n","        [7, 8]])\n","t.view(8):\n"," tensor([1, 2, 3, 4, 5, 6, 7, 8])\n"]}],"source":["# reshaping the tensor\n","\n","# there are two ways to reshape a tensor.\n","# 1. Using the .reshape() method\n","# 2. Using the .view() method\n","\n","# the difference between the two methods is that .view() only works when the tensor is contiguous in memory (C contiguous layout)\n","# whereas .reshape() will work regardless of the tensor's memory layout\n","\n","# in general, reshape() is flexible\n","\n","# create a 3d tensor\n","t = torch.tensor([\n","    [\n","        [1, 2],\n","        [3, 4]\n","    ],\n","    [\n","        [5, 6],\n","        [7, 8]\n","    ]\n","])\n","\n","# reshape the tensor into a 2x4 matrix\n","print(\"t.reshape(2, 4):\\n\", t.reshape(2, 4))\n","\n","# reshape the tensor into a 4x2 matrix\n","print(\"t.reshape(4, 2):\\n\", t.reshape(4, 2))\n","\n","# reshape the tensor into a 1D array\n","print(\"t.reshape(8):\\n\", t.reshape(8))\n","\n","# view the tensor as a 2x4 matrix\n","print(\"t.view(2, 4):\\n\", t.view(2, 4))\n","\n","# view the tensor as a 4x2 matrix\n","print(\"t.view(4, 2):\\n\", t.view(4, 2))\n","\n","# view the tensor as a 1D array\n","print(\"t.view(8):\\n\", t.view(8))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45541,"status":"ok","timestamp":1703144889004,"user":{"displayName":"Sai P","userId":"15285502759182123980"},"user_tz":480},"id":"UkqydI0JUQBv","outputId":"c6d77df7-aaf2-4fc0-8a5b-6945cc3bc7a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Available GPUs with name and memory:\n","Tesla T4 15835660288\n","Speedup factor: 9615.26481979559\n"]}],"source":["# CPU vs GPU\n","\n","# tensors can live on the CPU or the GPU.\n","# in cpu, tensors are stored as numpy arrays. in gpu, tensors are stored as cuda arrays\n","# the cuda array is similar to a numpy array, but it can be used on the GPU to accelerate computing\n","# .to() method can be used to move tensors from cpu to gpu and vice versa\n","# it is important to note that the .to() method returns a new tensor. it does not overwrite the original tensor\n","# if one tensor is on the cpu and another is on the gpu, we cannot perform operations between them. they must be on the same device (cpu or gpu)\n","# usually, we offload the bulk of the computation to the gpu, and then bring it back to the cpu to save it to disk\n","# does scikit-learn support gpu? no. scikit-learn is a python library that is built on top of numpy. numpy does not support gpu. therefore, scikit-learn does not support gpu\n","\n","# print with description all the available devices\n","import time\n","\n","print(\"Available GPUs with name and memory:\")\n","for i in range(torch.cuda.device_count()):\n","    print(torch.cuda.get_device_name(i), torch.cuda.get_device_properties(i).total_memory)\n","\n","# compute time taken to multiply and store it for later comparison\n","t_cpu = torch.ones(100, 100, 100)\n","start_time_cpu = time.time()\n","t_cpu @ t_cpu\n","end_time_cpu = time.time()\n","cpu_time = end_time_cpu - start_time_cpu\n","print(f'CPU time: {cpu_time}')\n","\n","# now calculate how much time it takes to move it to gpu and do the same multiplication\n","t_gpu = t_cpu.to('cuda')\n","\n","start_time_gpu = time.time()\n","t_gpu @ t_gpu\n","end_time_gpu = time.time()\n","gpu_time = end_time_gpu - start_time_gpu\n","print(f'GPU time: {gpu_time}')\n","\n","# print the factor of speedup from above\n","speedup_factor = cpu_time / gpu_time\n","print(f'Speedup factor: {speedup_factor}')\n","\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}