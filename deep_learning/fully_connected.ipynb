{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Goals\n",
    "- Create a feedforward neural network for classifying CIFAR10 dataset\n",
    "- Various activation functions and their pros/cons\n",
    "- Use tensor board for visualiazing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# imports and setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "writer = SummaryWriter('runs/cifar10_experiment_3')\n",
    "\n",
    "# Set random seed for pytorch and numpy\n",
    "# numpy seed takes care of numpy and scipy\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Using {device} device\")\n",
    "\n",
    "# classes from CIFAR10\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and preprocessing\n",
    "def imshow(ax, img):\n",
    "    npimg = img.numpy()\n",
    "    # normalize the image from 0 to 1, cifar10 has it from -1 to 1\n",
    "    npimg = (npimg+1)/2\n",
    "    # imshow expects color channel to be the third dimension\n",
    "    # and it expects the RGB values to be between \n",
    "    ax.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "def visualize_training_examples(train_dataset, classes, num_images=6):\n",
    "    # Select num_images random indices\n",
    "    indices = np.random.choice(len(train_dataset), size=num_images, replace=False)\n",
    "\n",
    "    # Show images and labels\n",
    "    # figure size should be dependent on \n",
    "    plt.figure(figsize=(8, (num_images//2)*4))\n",
    "    for i, idx in enumerate(indices):\n",
    "        ax = plt.subplot(num_images//2, 2, i + 1)\n",
    "        image, label = train_dataset[idx]\n",
    "        imshow(ax, image)\n",
    "        ax.set_title(classes[label])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def load_data(dataset_loc='./data',batch_size=64):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))  # Adjust these values for normalization\n",
    "    ])\n",
    "\n",
    "    # Dataset class stores features and target\n",
    "    # DataLoader builds an iterator on top of Dataset class\n",
    "\n",
    "    # Load CIFAR10 dataset\n",
    "    train_dataset = datasets.CIFAR10(root=dataset_loc, train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.CIFAR10(root=dataset_loc, train=False, download=True, transform=transform)\n",
    "\n",
    "    # Use stratified sampling to split the train dataset into train and validation\n",
    "    train_dataset, validation_dataset = train_test_split(train_dataset, test_size=0.2, random_state=42, stratify=train_dataset.targets)\n",
    "\n",
    "    # Log some info about the dataset type and size\n",
    "    logging.info(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    logging.info(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "    # visualize_training_examples(train_dataset, classes, num_images=20)\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, validation_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class FullyConnectedNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FullyConnectedNN, self).__init__()\n",
    "        # nn.Linear autograd is already included\n",
    "        self.fc1 = nn.Linear(3 * 32 * 32, 500)  # CIFAR10 images are 32x32x3\n",
    "        self.fc2 = nn.Linear(500, 10)  # 10 classes in CIFAR10\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3 * 32 * 32)  # Flatten the images\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # move data to device for every iteration\n",
    "        # gpu memory is limited, so we can't move all data at once\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # zero the gradients, otherwise they will accumulate\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        accuracy = (output.argmax(dim=1) == target).float().mean()\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_accuracy += accuracy.item()\n",
    "        if (batch_idx+1) % 100 == 0: # +1 so that we don't print for 0th batch\n",
    "            # Print running loss and running accuracy every 100 batches, also print fraction of epoch completed\n",
    "            logging.info(f'Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {running_loss/100:.6f}\\tAccuracy: {running_accuracy/100:.6f}')\n",
    "            # logging.info(f'Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {running_loss:.6f}')\n",
    "            # Log running loss to tensorboard\n",
    "            writer.add_scalar('training loss', running_loss / 100, epoch * len(train_loader) + batch_idx)\n",
    "            # Log accuracy to tensorboard\n",
    "            writer.add_scalar('training accuracy', running_accuracy / 100, epoch * len(train_loader) + batch_idx)\n",
    "            # writer.add_scalar('training accuracy', accuracy, epoch * len(train_loader) + batch_idx)\n",
    "            running_loss = 0.0\n",
    "            running_accuracy = 0.0\n",
    "\n",
    "def test(model, device, test_loader, epoch=None, validation=False):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            # reduction='sum' means that we will get the sum of the loss instead of the mean\n",
    "            # item() gives the scalar value of the loss\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            # get the index of the max log-probability\n",
    "            # dim=1 means that we will get the max value for each row\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    if validation:\n",
    "        logging.info(f'Validation set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)')\n",
    "        writer.add_scalar('validation loss', test_loss, epoch)\n",
    "    else:\n",
    "        # log test loss and accuracy to tensorboard\n",
    "        logging.info(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)')\n",
    "        writer.add_scalar('test loss', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train dataset size: 40000\n",
      "INFO:root:Test dataset size: 10000\n",
      "INFO:root:Epoch: 0 [6336/40000 (16%)]\tLoss: 1.910954\tAccuracy: 0.335469\n",
      "INFO:root:Epoch: 0 [12736/40000 (32%)]\tLoss: 1.714843\tAccuracy: 0.392188\n",
      "INFO:root:Epoch: 0 [19136/40000 (48%)]\tLoss: 1.649604\tAccuracy: 0.422500\n",
      "INFO:root:Epoch: 0 [25536/40000 (64%)]\tLoss: 1.610980\tAccuracy: 0.437812\n",
      "INFO:root:Epoch: 0 [31936/40000 (80%)]\tLoss: 1.616893\tAccuracy: 0.436563\n",
      "INFO:root:Epoch: 0 [38336/40000 (96%)]\tLoss: 1.589508\tAccuracy: 0.448594\n",
      "INFO:root:Validation set: Average loss: 1.5979, Accuracy: 4486/10000 (45%)\n",
      "INFO:root:Epoch: 1 [6336/40000 (16%)]\tLoss: 1.487238\tAccuracy: 0.487500\n",
      "INFO:root:Epoch: 1 [12736/40000 (32%)]\tLoss: 1.466520\tAccuracy: 0.484531\n",
      "INFO:root:Epoch: 1 [19136/40000 (48%)]\tLoss: 1.489074\tAccuracy: 0.478438\n",
      "INFO:root:Epoch: 1 [25536/40000 (64%)]\tLoss: 1.482614\tAccuracy: 0.486250\n",
      "INFO:root:Epoch: 1 [31936/40000 (80%)]\tLoss: 1.499839\tAccuracy: 0.475625\n",
      "INFO:root:Epoch: 1 [38336/40000 (96%)]\tLoss: 1.485888\tAccuracy: 0.482969\n",
      "INFO:root:Validation set: Average loss: 1.5298, Accuracy: 4786/10000 (48%)\n",
      "INFO:root:Epoch: 2 [6336/40000 (16%)]\tLoss: 1.391225\tAccuracy: 0.518125\n",
      "INFO:root:Epoch: 2 [12736/40000 (32%)]\tLoss: 1.383675\tAccuracy: 0.519219\n",
      "INFO:root:Epoch: 2 [19136/40000 (48%)]\tLoss: 1.378933\tAccuracy: 0.523281\n",
      "INFO:root:Epoch: 2 [25536/40000 (64%)]\tLoss: 1.425779\tAccuracy: 0.502344\n",
      "INFO:root:Epoch: 2 [31936/40000 (80%)]\tLoss: 1.393829\tAccuracy: 0.517656\n",
      "INFO:root:Epoch: 2 [38336/40000 (96%)]\tLoss: 1.401826\tAccuracy: 0.523906\n",
      "INFO:root:Validation set: Average loss: 1.4821, Accuracy: 4918/10000 (49%)\n",
      "INFO:root:Epoch: 3 [6336/40000 (16%)]\tLoss: 1.290157\tAccuracy: 0.544844\n",
      "INFO:root:Epoch: 3 [12736/40000 (32%)]\tLoss: 1.315569\tAccuracy: 0.545000\n",
      "INFO:root:Epoch: 3 [19136/40000 (48%)]\tLoss: 1.325991\tAccuracy: 0.540781\n",
      "INFO:root:Epoch: 3 [25536/40000 (64%)]\tLoss: 1.320571\tAccuracy: 0.543594\n",
      "INFO:root:Epoch: 3 [31936/40000 (80%)]\tLoss: 1.345027\tAccuracy: 0.537031\n",
      "INFO:root:Epoch: 3 [38336/40000 (96%)]\tLoss: 1.321002\tAccuracy: 0.547188\n",
      "INFO:root:Validation set: Average loss: 1.5020, Accuracy: 4993/10000 (50%)\n",
      "INFO:root:Epoch: 4 [6336/40000 (16%)]\tLoss: 1.212077\tAccuracy: 0.578125\n",
      "INFO:root:Epoch: 4 [12736/40000 (32%)]\tLoss: 1.239119\tAccuracy: 0.581094\n",
      "INFO:root:Epoch: 4 [19136/40000 (48%)]\tLoss: 1.246080\tAccuracy: 0.574531\n",
      "INFO:root:Epoch: 4 [25536/40000 (64%)]\tLoss: 1.247922\tAccuracy: 0.568438\n",
      "INFO:root:Epoch: 4 [31936/40000 (80%)]\tLoss: 1.263195\tAccuracy: 0.563438\n",
      "INFO:root:Epoch: 4 [38336/40000 (96%)]\tLoss: 1.293378\tAccuracy: 0.555625\n",
      "INFO:root:Validation set: Average loss: 1.4571, Accuracy: 5100/10000 (51%)\n",
      "INFO:root:Epoch: 5 [6336/40000 (16%)]\tLoss: 1.153550\tAccuracy: 0.604062\n",
      "INFO:root:Epoch: 5 [12736/40000 (32%)]\tLoss: 1.182608\tAccuracy: 0.605625\n",
      "INFO:root:Epoch: 5 [19136/40000 (48%)]\tLoss: 1.208410\tAccuracy: 0.582187\n",
      "INFO:root:Epoch: 5 [25536/40000 (64%)]\tLoss: 1.209076\tAccuracy: 0.580781\n",
      "INFO:root:Epoch: 5 [31936/40000 (80%)]\tLoss: 1.209062\tAccuracy: 0.590313\n",
      "INFO:root:Epoch: 5 [38336/40000 (96%)]\tLoss: 1.207572\tAccuracy: 0.589844\n",
      "INFO:root:Validation set: Average loss: 1.5259, Accuracy: 5076/10000 (51%)\n",
      "INFO:root:Epoch: 6 [6336/40000 (16%)]\tLoss: 1.077766\tAccuracy: 0.629844\n",
      "INFO:root:Epoch: 6 [12736/40000 (32%)]\tLoss: 1.097427\tAccuracy: 0.613594\n",
      "INFO:root:Epoch: 6 [19136/40000 (48%)]\tLoss: 1.148556\tAccuracy: 0.613594\n",
      "INFO:root:Epoch: 6 [25536/40000 (64%)]\tLoss: 1.155613\tAccuracy: 0.604844\n",
      "INFO:root:Epoch: 6 [31936/40000 (80%)]\tLoss: 1.173012\tAccuracy: 0.605938\n",
      "INFO:root:Epoch: 6 [38336/40000 (96%)]\tLoss: 1.173067\tAccuracy: 0.602500\n",
      "INFO:root:Validation set: Average loss: 1.5149, Accuracy: 5190/10000 (52%)\n",
      "INFO:root:Epoch: 7 [6336/40000 (16%)]\tLoss: 1.017518\tAccuracy: 0.655469\n",
      "INFO:root:Epoch: 7 [12736/40000 (32%)]\tLoss: 1.008027\tAccuracy: 0.652813\n",
      "INFO:root:Epoch: 7 [19136/40000 (48%)]\tLoss: 1.045437\tAccuracy: 0.633750\n",
      "INFO:root:Epoch: 7 [25536/40000 (64%)]\tLoss: 1.106088\tAccuracy: 0.625000\n",
      "INFO:root:Epoch: 7 [31936/40000 (80%)]\tLoss: 1.148908\tAccuracy: 0.613906\n",
      "INFO:root:Epoch: 7 [38336/40000 (96%)]\tLoss: 1.108898\tAccuracy: 0.631250\n",
      "INFO:root:Validation set: Average loss: 1.5539, Accuracy: 5149/10000 (51%)\n",
      "INFO:root:Epoch: 8 [6336/40000 (16%)]\tLoss: 0.978415\tAccuracy: 0.671719\n",
      "INFO:root:Epoch: 8 [12736/40000 (32%)]\tLoss: 0.990299\tAccuracy: 0.661875\n",
      "INFO:root:Epoch: 8 [19136/40000 (48%)]\tLoss: 0.980286\tAccuracy: 0.662500\n",
      "INFO:root:Epoch: 8 [25536/40000 (64%)]\tLoss: 1.027643\tAccuracy: 0.657344\n",
      "INFO:root:Epoch: 8 [31936/40000 (80%)]\tLoss: 1.057645\tAccuracy: 0.635781\n",
      "INFO:root:Epoch: 8 [38336/40000 (96%)]\tLoss: 1.066969\tAccuracy: 0.637344\n",
      "INFO:root:Validation set: Average loss: 1.6266, Accuracy: 5088/10000 (51%)\n",
      "INFO:root:Epoch: 9 [6336/40000 (16%)]\tLoss: 0.891217\tAccuracy: 0.689844\n",
      "INFO:root:Epoch: 9 [12736/40000 (32%)]\tLoss: 0.925868\tAccuracy: 0.686562\n",
      "INFO:root:Epoch: 9 [19136/40000 (48%)]\tLoss: 0.975562\tAccuracy: 0.670469\n",
      "INFO:root:Epoch: 9 [25536/40000 (64%)]\tLoss: 0.977680\tAccuracy: 0.672656\n",
      "INFO:root:Epoch: 9 [31936/40000 (80%)]\tLoss: 0.984528\tAccuracy: 0.664219\n",
      "INFO:root:Epoch: 9 [38336/40000 (96%)]\tLoss: 1.028679\tAccuracy: 0.655781\n",
      "INFO:root:Validation set: Average loss: 1.6200, Accuracy: 5168/10000 (52%)\n",
      "INFO:root:Epoch: 10 [6336/40000 (16%)]\tLoss: 0.821986\tAccuracy: 0.715938\n",
      "INFO:root:Epoch: 10 [12736/40000 (32%)]\tLoss: 0.850471\tAccuracy: 0.705781\n",
      "INFO:root:Epoch: 10 [19136/40000 (48%)]\tLoss: 0.893829\tAccuracy: 0.704063\n",
      "INFO:root:Epoch: 10 [25536/40000 (64%)]\tLoss: 0.891616\tAccuracy: 0.698750\n",
      "INFO:root:Epoch: 10 [31936/40000 (80%)]\tLoss: 0.991041\tAccuracy: 0.675937\n",
      "INFO:root:Epoch: 10 [38336/40000 (96%)]\tLoss: 0.989624\tAccuracy: 0.674375\n",
      "INFO:root:Validation set: Average loss: 1.6277, Accuracy: 5157/10000 (52%)\n",
      "INFO:root:Epoch: 11 [6336/40000 (16%)]\tLoss: 0.823311\tAccuracy: 0.723281\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/saip/My Drive/machine-learning-fundamentals/deep_learning/fully_connected.ipynb Cell 6\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/saip/My%20Drive/machine-learning-fundamentals/deep_learning/fully_connected.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/saip/My%20Drive/machine-learning-fundamentals/deep_learning/fully_connected.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39m50\u001b[39m):  \u001b[39m# 10 epochs\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/saip/My%20Drive/machine-learning-fundamentals/deep_learning/fully_connected.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     train(model, device, train_loader, optimizer, epoch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/saip/My%20Drive/machine-learning-fundamentals/deep_learning/fully_connected.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     test(model, device, validation_loader, epoch, validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/saip/My%20Drive/machine-learning-fundamentals/deep_learning/fully_connected.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m test(model, device, test_loader)\n",
      "\u001b[1;32m/Users/saip/My Drive/machine-learning-fundamentals/deep_learning/fully_connected.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/saip/My%20Drive/machine-learning-fundamentals/deep_learning/fully_connected.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m data, target \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device), target\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/saip/My%20Drive/machine-learning-fundamentals/deep_learning/fully_connected.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# zero the gradients, otherwise they will accumulate\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/saip/My%20Drive/machine-learning-fundamentals/deep_learning/fully_connected.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/saip/My%20Drive/machine-learning-fundamentals/deep_learning/fully_connected.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m output \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/saip/My%20Drive/machine-learning-fundamentals/deep_learning/fully_connected.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(output, target)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_dynamo\u001b[39m.\u001b[39;49mdisable(fn, recursive)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_dynamo/eval_frame.py:328\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m dynamic_ctx\u001b[39m.\u001b[39m\u001b[39m__enter__\u001b[39m()\n\u001b[1;32m    327\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    329\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/optimizer.py:808\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mgrad \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    807\u001b[0m     \u001b[39mif\u001b[39;00m set_to_none:\n\u001b[0;32m--> 808\u001b[0m         p\u001b[39m.\u001b[39;49mgrad \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    809\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    810\u001b[0m         \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mgrad_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "batch_size = 64\n",
    "dataset_loc = '/Users/saip/My Drive/machine-learning-fundamentals/datasets'\n",
    "train_loader, validation_loader, test_loader = load_data(dataset_loc, batch_size)\n",
    "model = FullyConnectedNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(0,50):  # 10 epochs\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, validation_loader, epoch, validation=True)\n",
    "\n",
    "test(model, device, test_loader)\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accuracy for this model is just 50%. The state of the art stands at 99.5%\n",
    "- Validation loss keeps increasing for every epoch while training loss keeps decreasing and becomes stagnant. \n",
    "  - The model is trying to learn the noise to decrease the training loss. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
