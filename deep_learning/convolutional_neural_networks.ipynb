{"cells":[{"cell_type":"markdown","metadata":{"id":"Z55g0MR57BUQ"},"source":["##Goals\n","- Create a convolutional network for classifying CIFAR10 dataset\n","- Play with model architecture to achieve maximum accuracy\n","- Optimize hyperparameters to achieve maximum accuracy\n","- Use tensor board for visualiazing"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13110,"status":"ok","timestamp":1703812211573,"user":{"displayName":"Sai P","userId":"15285502759182123980"},"user_tz":480},"id":"ByC8XGUz7BUS","outputId":"dbaefeca-341f-45e3-c1cc-1cdc5252bc71"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/saip/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n","  warnings.warn(\n","2023-12-30 15:49:19,830 - __main__ - INFO - Using cpu device\n"]}],"source":["# imports and setup\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","import torch.nn.functional as F\n","import logging\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from torch.utils.tensorboard import SummaryWriter\n","from sklearn.model_selection import train_test_split\n","import time\n","\n","# Create a custom logger so that we don't affect the root logger\n","logger = logging.getLogger(__name__)\n","logger.setLevel(logging.INFO)\n","# stop propagating to the root logger to avoid duplicate logs\n","logger.propagate = False\n","\n","# Create stdout handler, we can create another handler to write to a file if needed\n","ch = logging.StreamHandler()\n","ch.setLevel(logging.INFO)\n","formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n","ch.setFormatter(formatter)\n","\n","# Create a file handler\n","# use current time to create a unique log file name\n","\n","# Create a name for the experiment based on the current time and date\n","exp_name = time.strftime(\"%Y-%m-%d_%H-%M-%S\", time.localtime())\n","fh = logging.FileHandler(f'logs/{__name__}_{exp_name}.log')\n","fh.setLevel(logging.INFO)\n","fh.setFormatter(formatter)\n","\n","# add the handlers to the logger\n","logger.addHandler(ch)\n","logger.addHandler(fh)\n","\n","# create a tensorboard summary writer\n","writer = SummaryWriter('tensorboard_logs/' + exp_name)\n","\n","# Set random seed for pytorch and numpy\n","# numpy seed takes care of numpy and scipy\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","# Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","logger.info(f\"Using {device} device\")\n","\n","# classes from CIFAR10\n","classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# use a dictionary to store the hyperparameters\n","# this way we can easily pass them to the model and optimizer\n","\n","hyperparams = {\n","    'batch_size': 64,\n","    'num_epochs': 20,\n","    'learning_rate': 0.001,\n","    #'momentum': 0.9, # momentum is used to avoid local minima by taking into account the previous gradients\n","    #'weight_decay': 0.0001\n","}"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1703812211573,"user":{"displayName":"Sai P","userId":"15285502759182123980"},"user_tz":480},"id":"pPpJE8te7BUT"},"outputs":[],"source":["# Data loading and preprocessing\n","def imshow(ax, img):\n","    npimg = img.numpy()\n","    # normalize the image from 0 to 1, cifar10 has it from -1 to 1\n","    npimg = (npimg+1)/2\n","    # imshow expects color channel to be the third dimension\n","    # and it expects the RGB values to be between\n","    ax.imshow(np.transpose(npimg, (1, 2, 0)))\n","\n","def visualize_training_examples(train_dataset, classes, num_images=6):\n","    # Select num_images random indices\n","    indices = np.random.choice(len(train_dataset), size=num_images, replace=False)\n","\n","    # Show images and labels\n","    # figure size should be dependent on\n","    plt.figure(figsize=(8, (num_images//2)*4))\n","    for i, idx in enumerate(indices):\n","        ax = plt.subplot(num_images//2, 2, i + 1)\n","        image, label = train_dataset[idx]\n","        imshow(ax, image)\n","        ax.set_title(classes[label])\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","\n","def load_data(dataset_loc='./data',batch_size=64):\n","    transform = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5,), (0.5,))  # Adjust these values for normalization\n","    ])\n","\n","    # Dataset class stores features and target\n","    # DataLoader builds an iterator on top of Dataset class\n","\n","    # Load CIFAR10 dataset\n","    train_dataset = datasets.CIFAR10(root=dataset_loc, train=True, download=True, transform=transform)\n","    test_dataset = datasets.CIFAR10(root=dataset_loc, train=False, download=True, transform=transform)\n","\n","    # Use stratified sampling to split the train dataset into train and validation\n","    train_dataset, validation_dataset = train_test_split(train_dataset, test_size=0.2, random_state=42, stratify=train_dataset.targets)\n","\n","    # Log some info about the dataset type and size\n","    logger.info(f\"Train dataset size: {len(train_dataset)}\")\n","    logger.info(f\"Test dataset size: {len(test_dataset)}\")\n","\n","    # visualize_training_examples(train_dataset, classes, num_images=20)\n","\n","    # Data loaders\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    return train_loader, validation_loader, test_loader"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1703812211573,"user":{"displayName":"Sai P","userId":"15285502759182123980"},"user_tz":480},"id":"RqaDIcMM7BUU"},"outputs":[],"source":["class ConvolutionalNN(nn.Module):\n","    def __init__(self):\n","        super(ConvolutionalNN, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n","        self.bn1 = nn.BatchNorm2d(32)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n","        self.bn2 = nn.BatchNorm2d(64)\n","        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n","        self.bn3 = nn.BatchNorm2d(128)\n","        self.fc1 = nn.Linear(128 * 32 * 32, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.bn1(self.conv1(x)))\n","        x = F.relu(self.bn2(self.conv2(x)))\n","        x = F.relu(self.bn3(self.conv3(x)))\n","        x = x.view(x.size(0), -1)  # Flatten the tensor\n","        x = self.fc1(x)\n","        return x"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1703812211573,"user":{"displayName":"Sai P","userId":"15285502759182123980"},"user_tz":480},"id":"LHGlp2Ud7BUU"},"outputs":[],"source":["def train(model, device, train_loader, optimizer, epoch):\n","    model.train()\n","    running_loss = 0.0\n","    running_accuracy = 0.0\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        # move data to device for every iteration\n","        # gpu memory is limited, so we can't move all data at once\n","        data, target = data.to(device), target.to(device)\n","        # zero the gradients, otherwise they will accumulate\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = F.cross_entropy(output, target)\n","        accuracy = (output.argmax(dim=1) == target).float().mean()\n","        # compute gradients\n","        loss.backward()\n","        # update weights\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        running_accuracy += accuracy.item()\n","        if (batch_idx+1) % 100 == 0: # +1 so that we don't print for 0th batch\n","            # Print running loss and running accuracy every 100 batches, also print fraction of epoch completed\n","            logger.info(f'Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {running_loss/100:.6f}\\tAccuracy: {running_accuracy/100:.6f}')\n","            # logger.info(f'Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {running_loss:.6f}')\n","            # Log running loss to tensorboard\n","            writer.add_scalar('training loss', running_loss / 100, epoch * len(train_loader) + batch_idx)\n","            # Log accuracy to tensorboard\n","            writer.add_scalar('training accuracy', running_accuracy / 100, epoch * len(train_loader) + batch_idx)\n","            # writer.add_scalar('training accuracy', accuracy, epoch * len(train_loader) + batch_idx)\n","            running_loss = 0.0\n","            running_accuracy = 0.0\n","\n","def test(model, device, test_loader, epoch=None, validation=False):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            # reduction='sum' means that we will get the sum of the loss instead of the mean\n","            # item() gives the scalar value of the loss\n","            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n","            # get the index of the max log-probability\n","            # dim=1 means that we will get the max value for each row\n","            pred = output.argmax(dim=1)\n","            correct += pred.eq(target).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    if validation:\n","        logger.info(f'Validation set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)')\n","        writer.add_scalar('validation loss', test_loss, epoch)\n","        writer.add_scalar('validation accuracy', correct / len(test_loader.dataset), epoch)\n","    else:\n","        # log test loss and accuracy to tensorboard\n","        logger.info(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)')\n","        writer.add_scalar('test loss', test_loss)\n","        writer.add_scalar('test accuracy', correct / len(test_loader.dataset))"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":202902,"status":"ok","timestamp":1703812414470,"user":{"displayName":"Sai P","userId":"15285502759182123980"},"user_tz":480},"id":"mkkPM0un7BUU","outputId":"6f285495-8ee9-4abb-98e6-62aaf5171f50"},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]},{"name":"stderr","output_type":"stream","text":["2023-12-30 15:49:23,957 - __main__ - INFO - Train dataset size: 40000\n","2023-12-30 15:49:23,958 - __main__ - INFO - Test dataset size: 10000\n","2023-12-30 15:49:23,969 - __main__ - INFO - Model summary: \n","ConvolutionalNN(\n","  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (fc1): Linear(in_features=131072, out_features=10, bias=True)\n",")\n","2023-12-30 15:49:23,969 - __main__ - INFO - Number of trainable parameters: 1404426\n","2023-12-30 15:49:23,969 - __main__ - INFO - Hyperparameters: {'batch_size': 64, 'num_epochs': 20, 'learning_rate': 0.001}\n","2023-12-30 15:49:47,152 - __main__ - INFO - Epoch: 0 [6336/40000 (16%)]\tLoss: 12.642294\tAccuracy: 0.259219\n","2023-12-30 15:50:10,153 - __main__ - INFO - Epoch: 0 [12736/40000 (32%)]\tLoss: 4.716383\tAccuracy: 0.360938\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHyperparameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhyperparams\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, hyperparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m]):  \u001b[38;5;66;03m# 20 epochs\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     test(model, device, validation_loader, epoch, validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m test(model, device, test_loader)\n","Cell \u001b[0;32mIn[5], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# zero the gradients, otherwise they will accumulate\u001b[39;00m\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 11\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(output, target)\n\u001b[1;32m     13\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m (output\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m target)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[4], line 14\u001b[0m, in \u001b[0;36mConvolutionalNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))\n\u001b[0;32m---> 14\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x)))\n\u001b[1;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten the tensor\u001b[39;00m\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Main execution\n","dataset_loc = '/Users/saip/My Drive/machine-learning-fundamentals/datasets'\n","train_loader, validation_loader, test_loader = load_data(dataset_loc, hyperparams['batch_size'])\n","model = ConvolutionalNN().to(device)\n","optimizer = optim.Adam(model.parameters(), lr=hyperparams['learning_rate'])\n","# print the model summary along with the number of trainable parameters\n","logger.info(\"Model summary: \\n\" + str(model))\n","# print the number of trainable parameters\n","logger.info(f\"Number of trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n","\n","# print the hyperparameters\n","logger.info(f\"Hyperparameters: {hyperparams}\")\n","\n","for epoch in range(0, hyperparams['num_epochs']):  # 20 epochs\n","    train(model, device, train_loader, optimizer, epoch)\n","    test(model, device, validation_loader, epoch, validation=True)\n","\n","test(model, device, test_loader)\n","writer.close()\n"]},{"cell_type":"markdown","metadata":{},"source":["Achieved a validaiton accuracy of 68%. "]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}
