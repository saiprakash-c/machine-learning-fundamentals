{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Key Concepts in Implementing Collaborative Filtering in Python\n",
    "\n",
    "1. **Understanding Collaborative Filtering**: Grasp the fundamental concept of collaborative filtering, which leverages user behavior to recommend items. Learn about its types, such as user-based and item-based collaborative filtering.\n",
    "\n",
    "2. **Data Handling**: Learn to manipulate and process data in Python. Understand how to load, clean, and preprocess the dataset, including handling missing values and normalizing data.\n",
    "\n",
    "3. **Matrix Factorization Techniques**: Understand matrix factorization methods like Singular Value Decomposition (SVD) used in collaborative filtering to decompose a matrix into factors that can predict user preferences.\n",
    "\n",
    "4. **Similarity Metrics**: Learn about different similarity metrics like cosine similarity, Pearson correlation, and Jaccard similarity, which are crucial in comparing user or item profiles.\n",
    "\n",
    "5. **Building Recommendation Systems**: Learn to build a recommender system, focusing on generating user-item matrices, computing similarities, and making predictions.\n",
    "\n",
    "6. **Evaluation Metrics**: Understand various evaluation metrics such as Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and Precision at K, which are essential for assessing the performance of your recommendation system.\n",
    "\n",
    "7. **Handling Sparse Matrices**: Learn techniques to handle sparse matrices efficiently, as collaborative filtering often deals with large, sparse datasets.\n",
    "\n",
    "8.  **Scalability and Performance Issues**: Learn about scalability and performance considerations, such as handling large datasets and improving computational efficiency, which are crucial for real-world applications.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  User-ID  Age\n",
      "0       1  NaN\n",
      "1       2   18\n",
      "2       3  NaN\n",
      "3       4   17\n",
      "4       5  NaN\n",
      "         ISBN                                              Title  \\\n",
      "0  0195153448                                Classical Mythology   \n",
      "1  0002005018                                       Clara Callan   \n",
      "2  0060973129                               Decision in Normandy   \n",
      "3  0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
      "4  0393045218                             The Mummies of Urumchi   \n",
      "\n",
      "                 Author  Year                Publisher  \n",
      "0    Mark P. O. Morford  2002  Oxford University Press  \n",
      "1  Richard Bruce Wright  2001    HarperFlamingo Canada  \n",
      "2          Carlo D'Este  1991          HarperPerennial  \n",
      "3      Gina Bari Kolata  1999     Farrar Straus Giroux  \n",
      "4       E. J. W. Barber  1999   W. W. Norton & Company  \n",
      "   User-ID        ISBN  Rating\n",
      "0   276725  034545104X       0\n",
      "1   276726  0155061224       5\n",
      "2   276727  0446520802       0\n",
      "3   276729  052165615X       3\n",
      "4   276729  0521795028       6\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Take file names as input\n",
    "datasets_path = '/Users/saip/My Drive/machine-learning-fundamentals/datasets/'\n",
    "users_file = datasets_path + 'books_crossings/Users.csv'\n",
    "books_file = datasets_path + 'books_crossings/Books.csv'\n",
    "ratings_file = datasets_path + 'books_crossings/Ratings.csv'\n",
    "\n",
    "# Load the data\n",
    "users = pd.read_csv(users_file, sep=';', encoding='latin-1', low_memory=False)\n",
    "books = pd.read_csv(books_file, sep=';', encoding='latin-1', low_memory=False)\n",
    "ratings = pd.read_csv(ratings_file, sep=';', encoding='latin-1', low_memory=False)\n",
    "\n",
    "# Print the first few rows of each dataframe\n",
    "print(users.head())\n",
    "print(books.head())\n",
    "print(ratings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 278859\n",
      "Number of books: 271379\n",
      "Number of ratings: 1149780\n",
      "Average number of ratings per user: 10.920851419507423\n",
      "Average number of ratings per book: 3.376184827164989\n",
      "Sparsity of the rating matrix: 0.9984806639364702\n"
     ]
    }
   ],
   "source": [
    "# Explore the data\n",
    "\n",
    "# Check the number of users, books and ratings\n",
    "print('Number of users: {}'.format(len(users)))\n",
    "print('Number of books: {}'.format(len(books)))\n",
    "print('Number of ratings: {}'.format(len(ratings)))\n",
    "\n",
    "# Average number of ratings per user\n",
    "print('Average number of ratings per user: {}'.format(ratings['User-ID'].value_counts().mean()))\n",
    "\n",
    "# Average number of ratings per book\n",
    "print('Average number of ratings per book: {}'.format(ratings['ISBN'].value_counts().mean()))\n",
    "\n",
    "# Sparsity of the rating matrix in %\n",
    "sparsity = 1 - ((len(ratings) * 100) / (len(users) * len(books))) \n",
    "print('Sparsity of the rating matrix: {}'.format(sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 105283\n",
      "Number of unique books: 340556\n",
      "Size of ratings matrix: 35854757348\n",
      "Size of ratings matrix in GB: 267.13875940442085\n"
     ]
    }
   ],
   "source": [
    "# count the number of unique users and books in ratings table\n",
    "n_users = ratings['User-ID'].nunique()\n",
    "n_books = ratings['ISBN'].nunique()\n",
    "\n",
    "print('Number of unique users: {}'.format(n_users))\n",
    "print('Number of unique books: {}'.format(n_books))\n",
    "\n",
    "# get the size of ratings matrix\n",
    "ratings_matrix_size = n_users * n_books\n",
    "print('Size of ratings matrix: {}'.format(ratings_matrix_size))\n",
    "\n",
    "# estimate the size in GB occupied by the ratings matrix assuming each rating is a float number\n",
    "ratings_matrix_size_in_bytes = ratings_matrix_size * 8\n",
    "ratings_matrix_size_in_gb = ratings_matrix_size_in_bytes / (1024**3)\n",
    "print('Size of ratings matrix in GB: {}'.format(ratings_matrix_size_in_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot fit such a huge matrix into a 16 GB RAM. But scikit-surprise handles this internally by creating a sparse representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    3.4999  3.5002  3.5026  3.5046  3.4998  3.5014  0.0019  \n",
      "MAE (testset)     2.8143  2.8128  2.8147  2.8191  2.8155  2.8153  0.0021  \n",
      "Fit time          9.14    9.20    9.18    9.42    9.38    9.26    0.11    \n",
      "Test time         1.03    1.12    1.14    0.88    0.89    1.01    0.11    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([3.49985579, 3.50016134, 3.50256795, 3.50457273, 3.49981268]),\n",
       " 'test_mae': array([2.81428518, 2.81283864, 2.81468618, 2.81906326, 2.81549236]),\n",
       " 'fit_time': (9.139102935791016,\n",
       "  9.198290348052979,\n",
       "  9.181260108947754,\n",
       "  9.420634269714355,\n",
       "  9.377026081085205),\n",
       " 'test_time': (1.0309600830078125,\n",
       "  1.1181998252868652,\n",
       "  1.1370129585266113,\n",
       "  0.8779547214508057,\n",
       "  0.890427827835083)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use scikit-surprise to build a recommender system using collaborative filtering\n",
    "\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "# Load the data from the file using a reader and the load_from_df method\n",
    "reader = Reader(rating_scale=(0, 10))\n",
    "data = Dataset.load_from_df(ratings[['User-ID', 'ISBN', 'Rating']], reader)\n",
    "\n",
    "# Use the SVD algorithm.\n",
    "algo = SVD()\n",
    "\n",
    "# Run 5-fold cross-validation and print results.\n",
    "cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
