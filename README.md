# machine-learning-fundamentals
Implementation of machine learning algorithms in python using scikit-learn and pytorch. 

### 1. Introduction to PyTorch 
   - [x] **Understanding Tensors**: Basic building blocks in PyTorch.
   - [x] **Tensor Operations**: Addition, multiplication, and other operations.
   - [x] **Autograd**: PyTorch's automatic differentiation engine.
   - [x] **Backpropagation Basics**: Understanding how gradients are computed.
   - [x] **Simple Neural Network**: Building a basic neural network.
   - [ ] **Loss Functions**: Implementing mean squared error and cross-entropy.
   - [x] **Optimizers**: Using optimizers like SGD and Adam.
   - [x] **Data Loading**: Using `DataLoader` and `Dataset`.
   - [x] **CPU vs GPU Computation**: Running operations on different devices.
   - [ ] **Debugging**: Basic debugging in PyTorch.

### 2. Linear Regression with PyTorch
   - [x] **Model Definition**: Creating a linear regression model.
   - [x] **Batch Processing**: Processing data in batches.
   - [x] **Parameter Tuning**: Adjusting learning rate and other hyperparameters.
   - [x] **Evaluation Metrics**: MSE and RÂ² for regression.
   - [x] **Overfitting & Underfitting**: Basic concepts.
   - [x] **Regularization Techniques**: L1 and L2 regularization.
   - [x] **Saving & Loading Models**: Managing model states.
   - [x] **Visualization**: Plotting loss and predictions.
   - [x] **Feature Engineering**: Preparing data for linear regression.
   - [x] **Cross-Validation**: Implementing basic cross-validation.

### 3. Logistic Regression for Classification
   - [ ] **Binary Classification**: Basics of logistic regression.
   - [ ] **Sigmoid Function**: Understanding its role.
   - [ ] **Likelihood Function**: Maximizing log-likelihood.
   - [ ] **Confusion Matrix**: Analyzing model performance.
   - [ ] **Precision and Recall**: Understanding these metrics.
   - [ ] **ROC Curve**: Plotting and understanding AUC.
   - [ ] **Multiclass Classification**: Expanding to multiple classes.
   - [ ] **Softmax Function**: For multiclass logistic regression.
   - [ ] **Label Encoding**: Preparing categorical data.
   - [ ] **Class Imbalance**: Handling imbalanced datasets.

### 4. Feedforward Neural Networks
   - [x] **Network Architecture**: Designing a multi-layer network.
   - [ ] **Activation Functions**: ReLU, Sigmoid, Tanh.
   - [x] **Weight Initialization**: Xavier and He initialization.
   - [x] **Batch Normalization**: Accelerating training.
   - [x] **Dropout**: Regularizing the network.
   - [x] **Model Evaluation**: Using validation data.
   - [x] **Hyperparameter Optimization**: Grid search and random search.
   - [x] **Early Stopping**: Preventing overfitting.
   - [x] **Learning Rate Scheduling**: Adjusting the learning rate.
   - [x] **Transfer Learning**: Applying pre-trained networks.

### 5. Convolutional Neural Networks (CNNs)
   - [x] **Convolution Operation**: Understanding filters and feature maps.
   - [x] **Pooling Layers**: Max and average pooling.
   - [x] **CNN Architectures**: LeNet, AlexNet, etc.
   - [x] **Image Preprocessing**: Normalization and augmentation.
   - [ ] **Object Localization**: Basics of bounding boxes.
   - [ ] **Feature Visualization**: Understanding what CNNs learn.
   - [x] **Fine-Tuning**: Modifying pre-trained models.
   - [ ] **CAM (Class Activation Maps)**: Visualizing feature importance.
   - [ ] **Segmentation**: Basics of image segmentation.
   - [ ] **Advanced Architectures**: Introduction to ResNet, Inception, etc.

### 6. Recurrent Neural Networks (RNNs)
   - [ ] **RNN Basics**: Understanding RNN cells.
   - [ ] **Backpropagation Through Time (BPTT)**: How gradients are computed.
   - [ ] **Long Short-Term Memory (LSTM)**: Overcoming vanishing gradients.
   - [ ] **Gated Recurrent Units (GRUs)**: Simplified version of LSTM.
   - [ ] **Sequence Modeling**: Basics of text and sequence processing.
   - [ ] **Word Embeddings**: Using pre-trained embeddings.
   - [ ] **Language Modeling**: Building a basic language model.
   - [ ] **Teacher Forcing**: Technique in training RNNs.
   - [ ] **Attention Mechanisms**: Improving model performance.
   - [ ] **Seq2Seq Models**: Basics of machine translation.

### 7. Autoencoders
   - [ ] **Encoder-Decoder Architecture**: Basics of autoencoders.
   - [ ] **Latent Space Representation**: Understanding encoded features.
   - [ ] **Reconstruction Loss**: Measuring model performance.
   - [ ] **Regularized Autoencoders**: Sparse, denoising, and variational.
   - [ ] **Anomaly Detection**: Using autoencoders for outlier detection.
   - [ ] **Dimensionality Reduction**: Comparing with PCA.
   - [ ] **Data Generation**: Generating new data samples.
   - [ ] **Transfer Learning with Autoencoders**: Utilizing learned features.
   - [ ] **Deep Autoencoders**: Creating deeper models.
   - [ ] **Applications**: Practical uses in different domains.

### 8. Generative Adversarial Networks (GANs)
   - [ ] **GAN Basics**: Understanding generator and discriminator.
   - [ ] **Training GANs**: Challenges and techniques.
   - [ ] **Loss Functions**: For GAN training.
   - [ ] **Conditional GANs**: Controlling generated output.
   - [ ] **Image-to-Image Translation**: Basics of Pix2Pix and CycleGAN.
   - [ ] **Style Transfer**: Neural style transfer techniques.
   - [ ] **Evaluation Metrics**: Measuring GAN performance.
   - [ ] **Deep Convolutional GANs (DCGANs)**: Implementing DCGAN architecture.
   - [ ] **Advanced GANs**: Intro to BigGAN, StyleGAN, etc.
   - [ ] **Ethical Considerations**: Understanding the implications.

### 9. Reinforcement Learning with PyTorch
   - [ ] **RL Basics**: Understanding the RL framework.
   - [ ] **Markov Decision Processes (MDPs)**: Theoretical underpinnings.
   - [ ] **Q-Learning**: Introduction to value-based methods.
   - [ ] **Policy Gradient Methods**: Basics of policy optimization.
   - [ ] **Deep Q-Network (DQN)**: Combining RL with deep learning.
   - [ ] **Exploration vs Exploitation**: Strategies in RL.
   - [ ] **Actor-Critic Methods**: Combining value and policy methods.
   - [ ] **Multi-Agent RL**: Basics of cooperative and competitive environments.
   - [ ] **Simulated Environments**: Using OpenAI Gym.
   - [ ] **Real-World Applications**: Practical uses of RL.

### 10. Advanced Topics and Applications
   - [ ] **Transformer Models**: Understanding self-attention.
   - [ ] **BERT and GPT Models**: Basics of pre-trained language models.
   - [ ] **Graph Neural Networks (GNNs)**: Basics of graph-based learning.
   - [ ] **Time Series Analysis**: Using RNNs and CNNs.
   - [ ] **Federated Learning**: Privacy-preserving techniques.
   - [ ] **Explainable AI (XAI)**: Methods for model interpretability.
   - [ ] **Edge AI**: Deploying models on edge devices.
   - [ ] **Quantum Machine Learning**: Introduction to quantum computing in AI.
   - [ ] **Natural Language Processing (NLP)**: Advanced techniques.
   - [ ] **Computer Vision**: Recent trends and applications.

