{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"x2qqzM7gAP-u"},"outputs":[],"source":["# ensure python version is more than 3.5\n","import sys\n","assert sys.version_info >= (3,5)\n","\n","# ensure scikit version is >=0.20\n","import sklearn\n","assert sklearn.__version__ >= \"0.20\"\n","\n","import numpy as np\n","import os\n","\n","np.random.seed(42)"]},{"cell_type":"markdown","metadata":{"id":"6HS7k6HfM5hj"},"source":["##Data generation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VIqQFovFM9MQ"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.datasets import make_moons\n","\n","# generate some data for classification\n","X, y = make_moons(n_samples=500, noise=0.3, random_state=42)\n","# split the data for test and train\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"p2hi8TDbJNeE"},"source":["##Hard voting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0q3r4IRiJRLX"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# initialize three classifiers\n","lr_clf = LogisticRegression(solver='lbfgs',random_state=42)\n","rf_clf = RandomForestClassifier(n_estimators=100,random_state=42)\n","svm_clf = SVC(gamma='scale',random_state=42)\n","\n","# democratic voting classifier\n","voting_clf = VotingClassifier(estimators=[('lr',lr_clf),('rf',rf_clf),('svm',svm_clf)],voting='hard')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":110},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1698470389852,"user":{"displayName":"Sai P","userId":"15285502759182123980"},"user_tz":420},"id":"FZ60UuD2KZPM","outputId":"93672b35-9a0b-4f1b-bb54-3f83b0758d42"},"outputs":[],"source":["voting_clf.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":311,"status":"ok","timestamp":1698470390159,"user":{"displayName":"Sai P","userId":"15285502759182123980"},"user_tz":420},"id":"iBBDitBFKgB7","outputId":"daafd6b4-043e-456d-aacd-91e03526d22b"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","\n","for clf in (lr_clf, rf_clf, svm_clf, voting_clf):\n","  clf.fit(X_train, y_train)\n","  y_pred = clf.predict(X_test)\n","  print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"]},{"cell_type":"markdown","metadata":{"id":"rOI6HEGbT8WQ"},"source":["##Soft voting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sI28Y3bBUAKc"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# initialize three classifiers\n","lr_clf = LogisticRegression(solver='lbfgs',random_state=42)\n","rf_clf = RandomForestClassifier(n_estimators=100,random_state=42)\n","svm_clf = SVC(gamma='scale',probability=True, random_state=42)\n","\n","# democratic voting classifier\n","voting_clf = VotingClassifier(estimators=[('lr',lr_clf),('rf',rf_clf),('svm',svm_clf)],voting='soft')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":324,"status":"ok","timestamp":1698470390479,"user":{"displayName":"Sai P","userId":"15285502759182123980"},"user_tz":420},"id":"EZMVpD6LULPA","outputId":"783f0295-8507-42fa-c878-9a361414cc3a"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","\n","for clf in (lr_clf, rf_clf, svm_clf, voting_clf):\n","  clf.fit(X_train, y_train)\n","  y_pred = clf.predict(X_test)\n","  print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"]},{"cell_type":"markdown","metadata":{"id":"-d0HhE3jTSVN"},"source":["##Bagging ensembles"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":721,"status":"ok","timestamp":1698470391197,"user":{"displayName":"Sai P","userId":"15285502759182123980"},"user_tz":420},"id":"ZHQg55uPTU4l","outputId":"e6a56fe4-7340-4439-e2d6-ca7ac61bb5bc"},"outputs":[],"source":["from sklearn.ensemble import BaggingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n","                            max_samples=100, random_state=42)\n","bag_clf.fit(X_train, y_train)\n","y_pred = bag_clf.predict(X_test)\n","print('accuracy_score : {}'.format(accuracy_score(y_pred, y_test)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1698470391197,"user":{"displayName":"Sai P","userId":"15285502759182123980"},"user_tz":420},"id":"cO4FQmWLUK8_","outputId":"451fe428-0adc-4483-e2ad-7a62e0e8a7e9"},"outputs":[],"source":["dt_clf = DecisionTreeClassifier(random_state=42)\n","dt_clf.fit(X_train, y_train)\n","y_pred = dt_clf.predict(X_test)\n","print('accuracy_score: {}'.format(accuracy_score(y_pred, y_test)))"]},{"cell_type":"markdown","metadata":{"id":"duWxty5GtRSx"},"source":["## Random Forests"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2048,"status":"ok","timestamp":1698470409470,"user":{"displayName":"Sai P","userId":"15285502759182123980"},"user_tz":420},"id":"_JCTEarOtP_z","outputId":"3d9c79f0-9823-4f65-d1d4-f22cbd076827"},"outputs":[],"source":["# create a manual random forest\n","rf_clf = BaggingClassifier(DecisionTreeClassifier(max_features='sqrt'),\n","                           n_estimators=500, random_state=42)\n","rf_clf.fit(X_train, y_train)\n","y_pred = rf_clf.predict(X_test)\n","print('accuracy_score: {}'.format(accuracy_score(y_pred, y_test)))"]},{"cell_type":"markdown","metadata":{},"source":["## Feature Importance"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.datasets import fetch_openml\n","import matplotlib.pyplot as plt\n","\n","# Function to plot the feature importance\n","def plot_feature_importance(imp):\n","    plt.imshow(imp, cmap='hot', interpolation='nearest')\n","    plt.colorbar(label='Feature Importance')\n","    plt.title('Feature Importance of Each Pixel')\n","    plt.show()\n","\n","# load the data\n","mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n","\n","# set target as numerical type\n","mnist.target = mnist.target.astype(np.uint8)\n","\n","# train a random forest classifier\n","rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n","rnd_clf.fit(mnist[\"data\"], mnist[\"target\"])\n","\n","# plot the feature importance\n","imp = rnd_clf.feature_importances_\n","reshaped_imp = imp.reshape(28, 28)\n","plot_feature_importance(reshaped_imp)"]},{"cell_type":"markdown","metadata":{},"source":["## Gradient Boosting Regressor"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# create a synthetic dataset for regression with just two features\n","from sklearn.datasets import make_regression\n","\n","# n_features vs n_informative\n","X, y = make_regression(n_samples=1000, n_features=2, n_informative=2,\n","                       n_targets=1, noise=0.1, random_state=42)\n","\n","# Transform y to make the task non-linear\n","# For a cubic regression task:\n","y = y**3 + 0.1 * np.random.randn(1000)\n","\n","# normalize y\n","y = (y - y.mean()) / y.std()\n","\n","# split the data for train, validation and test (0.8, 0.1, 0.1)\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2,\n","                                                  random_state=42)\n","X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5,\n","                                                random_state=42)\n","\n","# print some X_train and y_train\n","print(X_train[:5])\n","print(y_train[:5])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# train gradient boosting regressor using decision tree regressor without using prebuilt model\n","# vary number of trees\n","from sklearn.tree import DecisionTreeRegressor\n","\n","learning_rate = 0.1 \n","num_trees = [5, 20, 100]\n","gb_regs = []\n","\n","for n in num_trees:\n","    residuals = y_train\n","    models = []\n","    for idx in range(n):\n","        single_reg = DecisionTreeRegressor(max_depth=2, random_state=42)\n","        single_reg.fit(X_train, residuals)\n","        # get residuals of y_train\n","        y_pred = single_reg.predict(X_train)\n","        residuals = residuals - learning_rate*y_pred\n","        models.append(single_reg)\n","    gb_regs.append(models)\n","\n","# To make predictions with this model, you would need to sum up the predictions from all the trees in the ensemble:\n","def predict(X, models):\n","    predictions = np.zeros(X.shape[0])\n","    for model in models:\n","        predictions += learning_rate * model.predict(X)\n","    return predictions\n","\n","# plot the predictions for both sizes of trees\n","import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(10, 5))\n","for idx, num in enumerate(num_trees):\n","    plt.subplot(1,len(num_trees),idx+1)\n","    plt.title('Prediction with {} trees'.format(num))\n","    plt.scatter(X_train[:, 0], y_train, c='b', label='train')\n","    # plot the fitted regression line\n","    y_pred = predict(X_train, gb_regs[idx])\n","    # Sort X_train and y_pred by the x-values\n","    sorted_indices = X_train[:, 0].argsort()\n","    X_train_sorted = X_train[sorted_indices]\n","    y_pred_sorted = y_pred[sorted_indices]\n","\n","    plt.plot(X_train_sorted[:, 0], y_pred_sorted, c='r', label='prediction')\n","    plt.legend()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# early stopping to find optimal number of trees\n","from sklearn.metrics import mean_squared_error\n","# import GradientBoostingRegressor\n","from sklearn.ensemble import GradientBoostingRegressor\n","\n","# initialize the model\n","gb_reg = GradientBoostingRegressor(max_depth=2, warm_start=True, random_state=42)\n","\n","# initialize the variables\n","val_errors = []\n","min_val_error = float('inf')\n","error_going_up = 0\n","best_num_trees = 0\n","\n","# train the model\n","for n_estimators in range(1, 120):\n","    gb_reg.n_estimators = n_estimators\n","    gb_reg.fit(X_train, y_train)\n","    y_pred = gb_reg.predict(X_val)\n","    val_error = mean_squared_error(y_val, y_pred)\n","    val_errors.append(val_error)\n","\n","# find the best number of trees\n","for idx, val_error in enumerate(val_errors):\n","    if val_error < min_val_error:\n","        min_val_error = val_error\n","        error_going_up = 0\n","        best_num_trees = idx+1\n","    else:\n","        error_going_up += 1\n","        if error_going_up == 5:\n","            break  # early stopping\n","\n","print('best number of trees: {}'.format(best_num_trees))\n","\n","# train the model with best number of trees\n","gb_reg = GradientBoostingRegressor(max_depth=2, n_estimators=best_num_trees,\n","                                   random_state=42)\n","gb_reg.fit(X_train, y_train)\n","y_pred = gb_reg.predict(X_test)\n","test_error = mean_squared_error(y_test, y_pred)\n","print('test error: {}'.format(test_error))\n","\n","# plot val_error with n_estimators and also plot the regression line \n","# for best number of trees\n","import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(10, 5))\n","plt.subplot(1,2,1)\n","plt.title('Validation error vs number of trees')\n","plt.plot(range(1, 120), val_errors, c='b')\n","plt.xlabel('n_estimators')\n","plt.ylabel('validation error')\n","plt.axvline(x=best_num_trees, c='r', linestyle='--')\n","plt.subplot(1,2,2)\n","plt.title('Prediction with {} trees'.format(best_num_trees))\n","plt.scatter(X_train[:, 0], y_train, c='b', label='train')\n","# plot the fitted regression line\n","y_pred = gb_reg.predict(X_train)\n","# Sort X_train and y_pred by the x-values\n","sorted_indices = X_train[:, 0].argsort()\n","X_train_sorted = X_train[sorted_indices]\n","y_pred_sorted = y_pred[sorted_indices]\n","plt.plot(X_train_sorted[:, 0], y_pred_sorted, c='r', label='prediction')\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"rklk4IukuPOA"},"source":["## Gradient Boosting Classifier - AdaBoost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XHPbsRaKuRp4"},"outputs":[],"source":["# train an adaboost classifier with decision tree as base estimator\n","from sklearn.ensemble import AdaBoostClassifier\n","\n","\n","# Function to plot the decision boundary for a binary classifier\n","def plot_decision_boundary(clf, X, y):\n","    # create a meshgrid\n","    x1s = np.linspace(np.min(X[:, 0]) - 0.5, np.max(X[:, 0]) + 0.5, 100)\n","    x2s = np.linspace(np.min(X[:, 1]) - 0.5, np.max(X[:, 1]) + 0.5, 100)\n","    \n","    # meshgrid function is used to create a rectangular grid out of two given one-dimensional arrays\n","    x1, x2 = np.meshgrid(x1s, x2s)\n","    # predict the output for the meshgrid\n","    y_pred = clf.predict(np.c_[x1.ravel(), x2.ravel()])\n","    y_pred = y_pred.reshape(x1.shape)\n","\n","    # plot the contour for the decision boundary\n","    # coutourf function is used to plot the contour for the decision boundary\n","    # it takes the x1, x2 and y_pred as input\n","    # cmap is the color map to use for the contour\n","    # alpha is the transparency of the contour\n","    plt.contourf(x1, x2, y_pred, cmap=plt.cm.brg, alpha=0.2)\n","    # plot the actual data\n","    plt.scatter(X[:, 0][y == 0], X[:, 1][y == 0], c='b', marker='^')\n","    plt.scatter(X[:, 0][y == 1], X[:, 1][y == 1], c='r', marker='o')\n","    plt.xlabel('X1')\n","    plt.ylabel('X2')\n","    plt.show()\n","\n","# algorithm is the hyperparameter that controls the boosting algorithm to use (SAMME or SAMME.R)\n","# SAMME.R relies on class probabilities rather than predictions and generally performs better\n","# SAMME is the default algorithm and relies on class predictions\n","# learning rate is the hyperparameter that controls how much each classifier is allowed to learn from the previous one\n","\n","# train an adaboost classifier\n","ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=200,\n","                             algorithm=\"SAMME.R\", learning_rate=0.5, random_state=42)\n","\n","ada_clf.fit(X_train, y_train)\n","\n","plot_decision_boundary(ada_clf, X, y)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}
